{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assister Discovery\n",
    "\n",
    "`Assister discovery` is a software that maps user requests to executable commands in [Assister Pipeline](https://github.com/keyvan-m-sadeghi/assister/tree/assister-conception/rfcs/text/assister-conception#assister-pipeline), illustrated in the following figure. Using [Natural Language Understanding](https://en.wikipedia.org/wiki/Natural-language_understanding) coupled with Machine Learning over the [Terms and Functions](https://github.com/keyvan-m-sadeghi/assister/tree/assister-conception/rfcs/text/assister-conception#terms-and-functions-language-tfx) contextual annotations embedded in an application, a discovery can translate a request to the corresponding command. The command will then be executed within the Pipeline. [[Link to the full proposal]](https://github.com/keyvan-m-sadeghi/assister/tree/assister-conception/rfcs/text/assister-conception)\n",
    "\n",
    "![Assister_pipeline](img/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Solution\n",
    "\n",
    "The research task corresponding to the aforementioned real problem at Assister, can be formally defined as:\n",
    "> `How can we map a user request in natural language to a pre-defined executable command?`\n",
    "\n",
    "If we had several mappings from user sentences to the requested commands, we would choose a [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) approach. This is not a realistic assumption at the current stage, although we know the possible commands (classes) at each context that can be selected for execution. So, we propose an [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) methology to tacke this probelm, as follows:\n",
    "1. Representation learning of user requests via [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).\n",
    "2. Finding the best embedding of each command description.\n",
    "3. Mapping a request to a command with the shortest [distance](https://en.wikipedia.org/wiki/Distance) in the embedding space based on a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure).\n",
    "\n",
    "It is obvious that word/sentence-level embedding algorithm is the key part of the solution. So, we first review the literature for this [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) task. Then, we describe the two recent state-of-the-art embedding models and how Assister utilizes them to embed the requests and comments. Finally, we explain the common similarity measures and pick a suitable one for Assister to calculate the distance between any pair of embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Undoubtedly, the year 2018 has been an inflection point for NLP after being relatively stationary for a couple of years. Word embedding is definitely one of the most popular representation of document [vocabulary](https://en.wikipedia.org/wiki/Vocabulary) with the capability of capturing words' [context](https://en.wikipedia.org/wiki/Context_(language_use)) in a document, syntactic and semantic similarity, relation between different words, etc. More formally, embeddings are low-dimensional representations of a data point (sample) in a higher-dimensional [vector space](https://en.wikipedia.org/wiki/Vector_space). In the same way, word embeddings are dense vectorized representations of words in a low-dimensional space. The first [neural network]()-based word embedding model was first proposed by Google in 2013 [[1]](#References). Since then, word embedding has received a lot of attention in almost every NLP model in practice. They are very effective, because by translating a word to an embedding one can model the semantic importance of a word in a numeric form and thus perform many [mathematical operations](https://en.wikipedia.org/wiki/Linear_algebra) on it. To make it more clear, let's take a look at a common example in the literature:\n",
    "> Let $\\phi$ be a word embedding mapping $W \\rightarrow \\mathbb{R}^n$, where $W$ is the word space and $\\mathbb{R}^n$ is an $n$-dimensional vector space, then we have:\n",
    ">\n",
    "> $\\phi(''king'') - \\phi(''man'') + \\phi(''woman'') = \\phi(''queen'')$\n",
    "\n",
    "It was first introduced by the [word2vec](https://code.google.com/archive/p/word2vec/) [[2]](#References) model in 2013 that was a great breakthrough. Another fascinating word embedding model was [Glove](https://nlp.stanford.edu/projects/glove/) [[3]](#References) in 2014. Although these two models are powerful, they are __context-free__ in which a single word embedding representation for each word in the vocabulary is generated. So, `bank` would have the same representation in `bank deposit` and `river bank`. Instead, __contextual__  models - including [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432) (2015) [[4]](#References), [Generative Pre-Training](https://openai.com/blog/language-unsupervised/) (2018) [[5]](#References), [ELMO](https://allennlp.org/elmo) (2018) [[6]](#References), [ULMFit](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html) (2018) [[7]](#References) - generate a representation of each word based on the other  words in the sentence, so they can capture both a static semantic meaning and a contextualized meaning. For instance, the word `apple` in the two sentences `I like apples` and `I like Apple macbooks` has a different semantic meaning, thus the embedding of this word would have a different vector representation which makes it more powerful for NLP tasks. The two recent state-of-the-art models - [USE](https://ai.google/research/pubs/pub46808) (2018) [[8]](#References) and [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) (2018) [[9]](#References) - use a powerful sequence transductive model for language understanding, called [Transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) (2017) [[10]](#References). We first review the Transformer model, then describe how the USE and the BERT models take the advantage of using Transformer as a building block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Today's NLP world benefits from the recent advancements of [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) research. More specifically, Google introduced a novel neural network architecture, called Transformer, in a seminal paper [[10]](#References) which outperformed many traditional [Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) sequence models (like [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) and [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)). The main advantages of using transformer as a language understanding unit is that (1) it can effectively model the long-term dependencies among words in a temporal word sequence; and (2) its [model training](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) phase is efficient by eliminating the sequential dependency on previous words [[10]](#References).\n",
    "\n",
    "A transformer is an [encoder-decoder architecture](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346) model that uses [attention mechanisms](https://skymind.ai/wiki/attention-mechanism-memory-network) to forward a complex pattern of the whole sequence to the decoder at once rather than sequentially as depicted in the following figure [[source](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)]:\n",
    "\n",
    "![Transformer](img/attention_path_length.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sequence Encoder (USE)\n",
    "\n",
    "The USE uses a transformer to provide sentence-level embeddings as easy as it has historically been to look up the embeddings for individual words, $e.g.$ word2vec. The universal sentence encoder is a model that encodes a text into 512-dimensional embeddings. The resulted embeddings can then be used as inputs to NLP tasks such as [sentiment classification](https://en.wikipedia.org/wiki/Sentiment_analysis) and [textual similarity](https://en.wikipedia.org/wiki/Semantic_similarity) analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Mikolov, T., I. Sutskever, K. Chen, G. Corrado and J. Dean, Distributed Representations of Words and Phrases and their Compositionality (2013)\n",
    "\n",
    "[2] Mikolov, T., Chen, K., Corrado, G., Dean, J., Sutskever, L. and Zweig, G., Efficient Estimation of Word Representations in Vector Space (2013)\n",
    "\n",
    "[3] Pennington, J., Socher, R. and Manning, C., Glove: Global vectors for word representation (2014)\n",
    "\n",
    "[4] Dai, A.M. and Le, Q.V., Semi-supervised sequence learning (2015)\n",
    "\n",
    "[5] Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I., Improving language understanding by generative pre-training (2018)\n",
    "\n",
    "[6] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. and Zettlemoyer, L., Deep contextualized word representations (2018)\n",
    "\n",
    "[7] Howard, J. and Ruder, S., Universal language model fine-tuning for text classification (2018)\n",
    "\n",
    "[8] Cer, D., Yang, Y., Kong, S.Y., Hua, N., Limtiaco, N., John, R.S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C. and Sung, Y.H., Universal sentence encoder (2018)\n",
    "\n",
    "[9] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., Bert: Pre-training of deep bidirectional transformers for language understanding (2018)\n",
    "\n",
    "[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.Gomez, L. Kaiser and I. Polosukhin, Attention Is All You Need (2017)\n",
    "\n",
    "[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)\n",
    "\n",
    "[2] Radford, Alec, Wu, Jeff, Child, Rewon, Luan, David, Amodei, Dario, Sutskever, Ilya, Language Models are Unsupervised Multitask Learners (2019)\n",
    "\n",
    "[3] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K.Lee and L.Zettlemoyer, Deep contextualized word representations (2018), North American Chapter of the Association for Computational Linguistics\n",
    "\n",
    "[6] Y. Wu, M. Schuster, Z. Chen, Q. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes and J. Dean Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
